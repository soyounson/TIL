이상적인 손실값 
* MSE 손실 
- 균형이 잡힌 GAN의 MSE의 손실은 0.25
* BCE 손실 
-이진 교차 엔트로피 손실은 확률과 불확실성에 기반 
-교차 엔트로피 : 실제 결과가 도출될 우도와 우리가 생각하는 우도 사이의 차이에 따른 결과의 불확실성에 대한 지표
-손실함수로 교차엔트로피 사용하는 이유 : MSE손실과는 다르게 교차 엔트로피는 로그 계산 덕분에 1.0보다 훨씬 커질수 있음
-`BCELoss()`를 쓸때 GAN의 이상적인 손실값은 0.693임

* GAN
-훈련데이터로 뭔가를 직접 기억하고 배우는 것이 아님
- 훈련데이터에서 나오는 우도 배움
- 생성기 또한 훈련 데이터를 직접적으로 보지 않음. 단지 추후에 참/거짓 판별할 수 있게끔 도와주는 역전파된 오차 
- 생성기는 우도 (likelihood) 혹은 확률 (probability)값들로 형성된 확률분포로 이미지 형성 시킴
- 확률분포로 이미지를 만들 떄 중요한 점은 단순히 이미지를 훈련 데이터에서 전체 혹은 일부를 복사하지 않는다는 점임. 대신 훈련 데이터 셋에서 관찰한 우도로 이미지 만듦. 
(하지만 픽셀 값 각각의 우도의 경우) 적합하지 않은 경우가 많이 발생. 
- 생성기 신경망 : 그 주변 픽셀을 고려하면서 색깔에 대한 우도를 정함. 

* GAN의 문제점 
> 많은 모드 & 모드 붕괴
모드 붕괴가 왔다면 생성기는 단 하나의 클래스에 대해서만 확률 분포 학습

* 합성곱, nnConv2d
- 스트라이드 : 커널이 얼마만큼의 보폭으로 움직이는 지 설정값
- 패딩 : 1로 설정함으로써 값을 0으로 설정한 테두리 만들수 있음. 즉, 이미지의 너비가 2로 늘어남. 커널은 이렇게 패딩을 넣은 후 늘어난 이미지에 대하여 적용.

* 전치합성곱, nn.ConvTranspose2d
- 텐서를 더 크게 만들기 위해 주로 쓰임. 
- 크기를 줄여가는 일반적인 합성곱과는 정확히 반대의 역활
- 일반적인 합성곱에서는 이미지 확장시 사용되지만, 여기서는 이미지 축소하는데 사용

* 경사하강법 (gradient descent)
- 손실함수가 학습 파라미터의 조합을 찾아 오차를 최호화
- 근본적으로 경사하강법은 GAN에는 맞지 않는 방법 
